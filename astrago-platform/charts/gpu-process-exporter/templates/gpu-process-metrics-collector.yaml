{{- if index .Values "gpu-process-exporter" "enabled" }}
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: gpu-process-exporter
  namespace: {{ .Release.Namespace }}
  labels:
    app: gpu-process-exporter
    chart: {{ include "gpu-process-exporter.chart" . }}
    release: {{ .Release.Name }}
    heritage: {{ .Release.Service }}
spec:
  selector:
    matchLabels:
      app: gpu-process-exporter
  template:
    metadata:
      labels:
        app: gpu-process-exporter
        release: {{ .Release.Name }}
    spec:
      nodeSelector:
        {{- toYaml .Values.nodeSelector | nindent 8 }}
      tolerations:
        {{- toYaml .Values.tolerations | nindent 8 }}
      hostPID: true
      containers:
      - name: gpu-process-exporter
        image: "{{ if .Values.offline }}{{ .Values.offline.registry | default "nvcr.io" }}{{ else }}nvcr.io{{ end }}/nvidia/k8s/dcgm-exporter:3.3.9-3.6.1-ubuntu22.04"
        ports:
        - name: metrics
          containerPort: {{ .Values.httpServer.port }}
          protocol: TCP
        env:
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: METRICS_PORT
          value: {{ .Values.httpServer.port | quote }}
        - name: COLLECTION_INTERVAL
          value: {{ .Values.httpServer.interval | quote }}
        command:
        - /bin/bash
        - -c
        - |
          set -e
          
          # Install necessary packages
          apt-get update && apt-get install -y python3 python3-pip curl
          pip3 install prometheus_client
          
          # Create metrics collection script
          cat > /tmp/gpu_metrics_server.py << 'EOF'
          #!/usr/bin/env python3
          import time
          import os
          import subprocess
          import json
          import re
          from prometheus_client import start_http_server, Gauge, Info
          from prometheus_client.core import CollectorRegistry
          import threading
          import logging
          
          # Configure logging
          logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
          logger = logging.getLogger(__name__)
          
          # Environment variables
          NODE_NAME = os.environ.get('NODE_NAME', 'unknown')
          METRICS_PORT = int(os.environ.get('METRICS_PORT', '8080'))
          COLLECTION_INTERVAL = int(os.environ.get('COLLECTION_INTERVAL', '30'))
          
          # Create custom registry
          registry = CollectorRegistry()
          
          # Define metrics
          gpu_process_count = Gauge(
              'gpu_process_count', 
              'Number of active GPU processes per GPU',
              ['gpu', 'gi_id', 'ci_id'],
              registry=registry
          )
          
          gpu_total_processes = Gauge(
              'gpu_total_processes',
              'Total number of GPU processes across all GPUs',
              registry=registry
          )
          
          gpu_process_utilization = Gauge(
              'gpu_process_utilization',
              'GPU utilization per process with detailed info',
              ['node', 'gpu', 'pid', 'command', 'pod', 'namespace', 'container', 'gpu_memory', 'start_time', 'gi_id', 'ci_id'],
              registry=registry
          )
          
          gpu_process_memory_utilization = Gauge(
              'gpu_process_memory_utilization',
              'GPU memory utilization per process',
              ['node', 'gpu', 'pid', 'command', 'pod', 'namespace', 'container', 'gpu_memory', 'start_time', 'gi_id', 'ci_id'],
              registry=registry
          )
          
          gpu_process_info = Gauge(
              'gpu_process_info',
              'GPU process information with Pod/Container details',
              ['node', 'gpu', 'pid', 'command', 'pod', 'namespace', 'container', 'status', 'gpu_memory', 'start_time', 'gi_id', 'ci_id'],
              registry=registry
          )
          
          class GPUMetricsCollector:
              def __init__(self):
                  self.lock = threading.Lock()
                  
              def run_nvidia_smi(self, query, index=None):
                  """Run nvidia-smi command and return output"""
                  try:
                      cmd = ['nvidia-smi']
                      if index is not None:
                          cmd.extend(['-i', str(index)])
                      cmd.extend(['--query-gpu=' + query, '--format=csv,noheader,nounits'])
                      
                      result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
                      if result.returncode == 0:
                          return result.stdout.strip()
                      else:
                          logger.error(f"nvidia-smi error: {result.stderr}")
                          return ""
                  except Exception as e:
                      logger.error(f"Error running nvidia-smi: {e}")
                      return ""
              
              def get_process_info(self, pid):
                  """Get process information from /proc"""
                  try:
                      # Process start time
                      stat_path = f"/host/proc/{pid}/stat"
                      if os.path.exists(stat_path):
                          with open(stat_path, 'r') as f:
                              stat_data = f.read().split()
                              start_time_jiffies = int(stat_data[21])
                              # Convert to epoch time (approximate)
                              start_time = int(time.time()) - (start_time_jiffies // 100)
                      else:
                          start_time = 0
                      
                      # Pod information from cgroup
                      pod_name = "unknown"
                      namespace = "unknown"
                      container_name = "unknown"
                      
                      cgroup_path = f"/host/proc/{pid}/cgroup"
                      if os.path.exists(cgroup_path):
                          with open(cgroup_path, 'r') as f:
                              cgroup_content = f.read()
                              if 'kubepods' in cgroup_content:
                                  # Extract pod UID from cgroup
                                  pod_match = re.search(r'pod([a-f0-9-]+)', cgroup_content)
                                  if pod_match:
                                      pod_name = f"pod-{pod_match.group(1)}"
                                      namespace = "detected"
                      
                      return {
                          'pod_name': pod_name,
                          'namespace': namespace,
                          'container_name': container_name,
                          'start_time': str(start_time)
                      }
                  except Exception as e:
                      logger.warning(f"Error getting process info for PID {pid}: {e}")
                      return {
                          'pod_name': 'unknown',
                          'namespace': 'unknown', 
                          'container_name': 'unknown',
                          'start_time': '0'
                      }
              
              def collect_metrics(self):
                  """Collect GPU process metrics"""
                  with self.lock:
                      try:
                          logger.info("Starting GPU metrics collection...")
                          
                          # Clear all metrics
                          gpu_process_count.clear()
                          gpu_process_utilization.clear()
                          gpu_process_memory_utilization.clear()
                          gpu_process_info.clear()
                          
                          # Check MIG mode
                          mig_mode = self.run_nvidia_smi('mig.mode.current')
                          mig_enabled = mig_mode and 'Enabled' in mig_mode
                          logger.info(f"MIG mode: {'Enabled' if mig_enabled else 'Disabled'}")
                          
                          # Get GPU list
                          gpu_indices = self.run_nvidia_smi('index').split('\n')
                          gpu_indices = [idx.strip() for idx in gpu_indices if idx.strip()]
                          
                          # Get GPU mapping
                          if mig_enabled:
                              gpu_mapping = {}
                              uuid_list = self.run_nvidia_smi('index,uuid').split('\n')
                              for line in uuid_list:
                                  if line.strip():
                                      parts = line.split(',')
                                      if len(parts) >= 2:
                                          gpu_mapping[parts[1].strip()] = parts[0].strip()
                          else:
                              gpu_mapping = {}
                              bus_list = self.run_nvidia_smi('index,pci.bus_id').split('\n')
                              for line in bus_list:
                                  if line.strip():
                                      parts = line.split(',')
                                      if len(parts) >= 2:
                                          gpu_mapping[parts[1].strip()] = parts[0].strip()
                          
                          # Get running processes
                          total_processes = 0
                          gpu_process_counts = {gpu_idx: 0 for gpu_idx in gpu_indices}
                          
                          try:
                              if mig_enabled:
                                  processes_output = subprocess.run([
                                      'nvidia-smi', '--query-compute-apps=gpu_uuid,pid,process_name,used_gpu_memory',
                                      '--format=csv,noheader,nounits'
                                  ], capture_output=True, text=True, timeout=30)
                              else:
                                  processes_output = subprocess.run([
                                      'nvidia-smi', '--query-compute-apps=gpu_bus_id,pid,process_name,used_gpu_memory', 
                                      '--format=csv,noheader,nounits'
                                  ], capture_output=True, text=True, timeout=30)
                              
                              if processes_output.returncode == 0 and processes_output.stdout.strip():
                                  process_lines = processes_output.stdout.strip().split('\n')
                                  
                                  for line in process_lines:
                                      if 'No running processes found' in line:
                                          continue
                                          
                                      parts = [p.strip() for p in line.split(',')]
                                      if len(parts) >= 4:
                                          gpu_identifier, pid, process_name, gpu_memory = parts[:4]
                                          
                                          if pid and pid != '-' and pid.isdigit():
                                              # Map GPU identifier to index
                                              gpu_index = gpu_mapping.get(gpu_identifier, '0')
                                              
                                              # Get process info
                                              proc_info = self.get_process_info(pid)
                                              
                                              # Get GPU utilization (simplified)
                                              gpu_util = 0
                                              mem_util = 0
                                              
                                              # Try to get utilization from nvidia-smi pmon
                                              try:
                                                  pmon_output = subprocess.run([
                                                      'nvidia-smi', 'pmon', '-i', gpu_index, '-s', 'um', '-c', '1'
                                                  ], capture_output=True, text=True, timeout=10)
                                                  
                                                  if pmon_output.returncode == 0:
                                                      pmon_lines = pmon_output.stdout.split('\n')[2:]  # Skip header
                                                      for pmon_line in pmon_lines:
                                                          if pid in pmon_line:
                                                              pmon_parts = pmon_line.split()
                                                              if len(pmon_parts) >= 10:
                                                                  try:
                                                                      gpu_util = float(pmon_parts[3]) if pmon_parts[3].isdigit() else 0
                                                                      used_mem = float(pmon_parts[9]) if pmon_parts[9].isdigit() else 0
                                                                      total_mem_str = self.run_nvidia_smi('memory.total', gpu_index)
                                                                      if total_mem_str.isdigit():
                                                                          total_mem = float(total_mem_str)
                                                                          mem_util = (used_mem / total_mem) * 100 if total_mem > 0 else 0
                                                                  except:
                                                                      pass
                                                              break
                                              except:
                                                  pass
                                              
                                              # Create labels
                                              labels = {
                                                  'node': NODE_NAME,
                                                  'gpu': gpu_index.replace('/', '_'),
                                                  'pid': pid,
                                                  'command': process_name,
                                                  'pod': proc_info['pod_name'],
                                                  'namespace': proc_info['namespace'],
                                                  'container': proc_info['container_name'],
                                                  'gpu_memory': f"{gpu_memory}MiB",
                                                  'start_time': proc_info['start_time'],
                                                  'gi_id': 'none',
                                                  'ci_id': 'none'
                                              }
                                              
                                              # Set metrics
                                              gpu_process_utilization.labels(**labels).set(gpu_util)
                                              gpu_process_memory_utilization.labels(**labels).set(mem_util)
                                              gpu_process_info.labels(**{**labels, 'status': 'active'}).set(1)
                                              
                                              gpu_process_counts[gpu_index] += 1
                                              total_processes += 1
                              
                          except Exception as e:
                              logger.error(f"Error collecting process information: {e}")
                          
                          # Set process counts for each GPU
                          for gpu_idx in gpu_indices:
                              count = gpu_process_counts.get(gpu_idx, 0)
                              gpu_process_count.labels(
                                  gpu=gpu_idx.replace('/', '_'),
                                  gi_id='none',
                                  ci_id='none'
                              ).set(count)
                              
                              # Set idle status for GPUs with no processes
                              if count == 0:
                                  gpu_process_info.labels(
                                      node=NODE_NAME,
                                      gpu=gpu_idx.replace('/', '_'),
                                      pid='none',
                                      command='none',
                                      pod='none',
                                      namespace='none',
                                      container='none',
                                      status='idle',
                                      gpu_memory='0MiB',
                                      start_time='0',
                                      gi_id='none',
                                      ci_id='none'
                                  ).set(0)
                          
                          # Set total processes
                          gpu_total_processes.set(total_processes)
                          
                          logger.info(f"Collected metrics for {len(gpu_indices)} GPUs, {total_processes} total processes")
                          
                      except Exception as e:
                          logger.error(f"Error in metrics collection: {e}")
              
              def start_collection_loop(self):
                  """Start the metrics collection loop"""
                  logger.info(f"Starting metrics collection loop (interval: {COLLECTION_INTERVAL}s)")
                  while True:
                      self.collect_metrics()
                      time.sleep(COLLECTION_INTERVAL)
          
          def main():
              logger.info(f"Starting GPU Process Exporter on port {METRICS_PORT}")
              
              # Create collector
              collector = GPUMetricsCollector()
              
              # Start HTTP server
              start_http_server(METRICS_PORT, registry=registry)
              logger.info(f"HTTP server started on port {METRICS_PORT}")
              
              # Initial collection
              collector.collect_metrics()
              
              # Start collection loop
              collector.start_collection_loop()
          
          if __name__ == '__main__':
              main()
          EOF
          
          # Make script executable and run
          chmod +x /tmp/gpu_metrics_server.py
          python3 /tmp/gpu_metrics_server.py
        securityContext:
          privileged: true
        resources:
          {{- toYaml .Values.httpServer.resources | nindent 10 }}
        volumeMounts:
        - name: host-proc
          mountPath: /host/proc
          readOnly: true
        livenessProbe:
          httpGet:
            path: /metrics
            port: {{ .Values.httpServer.port }}
          initialDelaySeconds: 30
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /metrics
            port: {{ .Values.httpServer.port }}
          initialDelaySeconds: 10
          periodSeconds: 10
      volumes:
      - name: host-proc
        hostPath:
          path: /proc

{{- if .Values.service.enabled }}
---
apiVersion: v1
kind: Service
metadata:
  name: gpu-process-exporter
  namespace: {{ .Release.Namespace }}
  labels:
    app: gpu-process-exporter
    chart: {{ include "gpu-process-exporter.chart" . }}
    release: {{ .Release.Name }}
    heritage: {{ .Release.Service }}
    {{- with .Values.service.labels }}
    {{- toYaml . | nindent 4 }}
    {{- end }}
  {{- with .Values.service.annotations }}
  annotations:
    {{- toYaml . | nindent 4 }}
  {{- end }}
spec:
  type: {{ .Values.service.type }}
  ports:
  - port: {{ .Values.service.port }}
    targetPort: {{ .Values.service.targetPort }}
    protocol: TCP
    name: metrics
  selector:
    app: gpu-process-exporter
{{- end }}

{{- if .Values.serviceMonitor.enabled }}
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: gpu-process-exporter
  namespace: {{ .Release.Namespace }}
  labels:
    app: gpu-process-exporter
    chart: {{ include "gpu-process-exporter.chart" . }}
    release: {{ .Release.Name }}
    heritage: {{ .Release.Service }}
    {{- with .Values.serviceMonitor.labels }}
    {{- toYaml . | nindent 4 }}
    {{- end }}
spec:
  selector:
    {{- toYaml .Values.serviceMonitor.selector | nindent 4 }}
  endpoints:
  - port: metrics
    interval: {{ .Values.serviceMonitor.interval }}
    scrapeTimeout: {{ .Values.serviceMonitor.scrapeTimeout }}
    path: /metrics
{{- end }}
{{- end }}
