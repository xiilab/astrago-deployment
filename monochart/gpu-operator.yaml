---
# Source: gpu-operator/charts/node-feature-discovery/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: node-feature-discovery
  namespace: gpu-operator
  labels:
    helm.sh/chart: node-feature-discovery-0.14.2
    app.kubernetes.io/name: node-feature-discovery
    app.kubernetes.io/instance: gpu-operator
    app.kubernetes.io/version: "v0.14.2"
    app.kubernetes.io/managed-by: Helm
---
# Source: gpu-operator/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: gpu-operator
  namespace: gpu-operator    
  labels:
    app.kubernetes.io/name: gpu-operator
    helm.sh/chart: gpu-operator-v23.9.0
    app.kubernetes.io/instance: gpu-operator
    app.kubernetes.io/version: "v23.9.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: "gpu-operator"
---
# Source: gpu-operator/charts/node-feature-discovery/templates/nfd-master-conf.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: gpu-operator-node-feature-discovery-master-conf
  namespace: gpu-operator
  labels:
    helm.sh/chart: node-feature-discovery-0.14.2
    app.kubernetes.io/name: node-feature-discovery
    app.kubernetes.io/instance: gpu-operator
    app.kubernetes.io/version: "v0.14.2"
    app.kubernetes.io/managed-by: Helm
data:
  nfd-master.conf: |-
    extraLabelNs:
    - nvidia.com
---
# Source: gpu-operator/charts/node-feature-discovery/templates/nfd-topologyupdater-conf.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: gpu-operator-node-feature-discovery-topology-updater-conf
  namespace: gpu-operator
  labels:
    helm.sh/chart: node-feature-discovery-0.14.2
    app.kubernetes.io/name: node-feature-discovery
    app.kubernetes.io/instance: gpu-operator
    app.kubernetes.io/version: "v0.14.2"
    app.kubernetes.io/managed-by: Helm
data:
  nfd-topology-updater.conf: |-
    null
---
# Source: gpu-operator/charts/node-feature-discovery/templates/nfd-worker-conf.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: gpu-operator-node-feature-discovery-worker-conf
  namespace: gpu-operator
  labels:
    helm.sh/chart: node-feature-discovery-0.14.2
    app.kubernetes.io/name: node-feature-discovery
    app.kubernetes.io/instance: gpu-operator
    app.kubernetes.io/version: "v0.14.2"
    app.kubernetes.io/managed-by: Helm
data:
  nfd-worker.conf: |-
    sources:
      pci:
        deviceClassWhitelist:
        - "02"
        - "0200"
        - "0207"
        - "0300"
        - "0302"
        deviceLabelFields:
        - vendor
---
# Source: gpu-operator/templates/metrics-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: metrics-config
  namespace: gpu-operator
data:
  dcgm-metrics.csv: |-
    # Format
    # If line starts with a '#' it is considered a comment
    # DCGM FIELD, Prometheus metric type, help message
    
    # Clocks
    DCGM_FI_DEV_SM_CLOCK,  gauge, SM clock frequency (in MHz).
    DCGM_FI_DEV_MEM_CLOCK, gauge, Memory clock frequency (in MHz).
    
    # Temperature
    DCGM_FI_DEV_MEMORY_TEMP, gauge, Memory temperature (in C).
    DCGM_FI_DEV_GPU_TEMP,    gauge, GPU temperature (in C).
    
    # Power
    DCGM_FI_DEV_POWER_USAGE,              gauge, Power draw (in W).
    DCGM_FI_DEV_TOTAL_ENERGY_CONSUMPTION, counter, Total energy consumption since boot (in mJ).
    DCGM_FI_DEV_FAN_SPEED,                gauge, Fan speed for the device in percent (in %).	
    
    # PCIE
    # DCGM_FI_DEV_PCIE_TX_THROUGHPUT,  counter, Total number of bytes transmitted through PCIe TX (in KB) via NVML.
    # DCGM_FI_DEV_PCIE_RX_THROUGHPUT,  counter, Total number of bytes received through PCIe RX (in KB) via NVML.
    DCGM_FI_DEV_PCIE_REPLAY_COUNTER, counter, Total number of PCIe retries.
    
    # Utilization (the sample period varies depending on the product)
    DCGM_FI_DEV_GPU_UTIL,      gauge, GPU utilization (in %).
    DCGM_FI_DEV_MEM_COPY_UTIL, gauge, Memory utilization (in %).
    DCGM_FI_DEV_ENC_UTIL,      gauge, Encoder utilization (in %).
    DCGM_FI_DEV_DEC_UTIL ,     gauge, Decoder utilization (in %).
    
    # Errors and violations
    DCGM_FI_DEV_XID_ERRORS,            gauge,   Value of the last XID error encountered.
    # DCGM_FI_DEV_POWER_VIOLATION,       counter, Throttling duration due to power constraints (in us).
    # DCGM_FI_DEV_THERMAL_VIOLATION,     counter, Throttling duration due to thermal constraints (in us).
    # DCGM_FI_DEV_SYNC_BOOST_VIOLATION,  counter, Throttling duration due to sync-boost constraints (in us).
    # DCGM_FI_DEV_BOARD_LIMIT_VIOLATION, counter, Throttling duration due to board limit constraints (in us).
    # DCGM_FI_DEV_LOW_UTIL_VIOLATION,    counter, Throttling duration due to low utilization (in us).
    # DCGM_FI_DEV_RELIABILITY_VIOLATION, counter, Throttling duration due to reliability constraints (in us).
    
    # Memory usage
    DCGM_FI_DEV_FB_FREE, gauge, Framebuffer memory free (in MiB).
    DCGM_FI_DEV_FB_USED, gauge, Framebuffer memory used (in MiB).
    
    # ECC
    # DCGM_FI_DEV_ECC_SBE_VOL_TOTAL, counter, Total number of single-bit volatile ECC errors.
    # DCGM_FI_DEV_ECC_DBE_VOL_TOTAL, counter, Total number of double-bit volatile ECC errors.
    # DCGM_FI_DEV_ECC_SBE_AGG_TOTAL, counter, Total number of single-bit persistent ECC errors.
    # DCGM_FI_DEV_ECC_DBE_AGG_TOTAL, counter, Total number of double-bit persistent ECC errors.
    
    # Retired pages
    # DCGM_FI_DEV_RETIRED_SBE,     counter, Total number of retired pages due to single-bit errors.
    # DCGM_FI_DEV_RETIRED_DBE,     counter, Total number of retired pages due to double-bit errors.
    # DCGM_FI_DEV_RETIRED_PENDING, counter, Total number of pages pending retirement.
    
    # NVLink
    # DCGM_FI_DEV_NVLINK_CRC_FLIT_ERROR_COUNT_TOTAL, counter, Total number of NVLink flow-control CRC errors.
    # DCGM_FI_DEV_NVLINK_CRC_DATA_ERROR_COUNT_TOTAL, counter, Total number of NVLink data CRC errors.
    # DCGM_FI_DEV_NVLINK_REPLAY_ERROR_COUNT_TOTAL,   counter, Total number of NVLink retries.
    # DCGM_FI_DEV_NVLINK_RECOVERY_ERROR_COUNT_TOTAL, counter, Total number of NVLink recovery errors.
    DCGM_FI_DEV_NVLINK_BANDWIDTH_TOTAL,            counter, Total number of NVLink bandwidth counters for all lanes.
    # DCGM_FI_DEV_NVLINK_BANDWIDTH_L0,               counter, The number of bytes of active NVLink rx or tx data including both header and payload.
    
    # VGPU License status
    DCGM_FI_DEV_VGPU_LICENSE_STATUS, gauge, vGPU License status
    
    # Remapped rows
    DCGM_FI_DEV_UNCORRECTABLE_REMAPPED_ROWS, counter, Number of remapped rows for uncorrectable errors
    DCGM_FI_DEV_CORRECTABLE_REMAPPED_ROWS,   counter, Number of remapped rows for correctable errors
    DCGM_FI_DEV_ROW_REMAP_FAILURE,           gauge,   Whether remapping of rows has failed
    
    # Static configuration information. These appear as labels on the other metrics
    DCGM_FI_DRIVER_VERSION,        label, Driver Version
    # DCGM_FI_NVML_VERSION,          label, NVML Version
    # DCGM_FI_DEV_BRAND,             label, Device Brand
    # DCGM_FI_DEV_SERIAL,            label, Device Serial Number
    # DCGM_FI_DEV_OEM_INFOROM_VER,   label, OEM inforom version
    # DCGM_FI_DEV_ECC_INFOROM_VER,   label, ECC inforom version
    # DCGM_FI_DEV_POWER_INFOROM_VER, label, Power management object inforom version
    # DCGM_FI_DEV_INFOROM_IMAGE_VER, label, Inforom image version
    # DCGM_FI_DEV_VBIOS_VERSION,     label, VBIOS version of the device
    
    # DCP metrics
    DCGM_FI_PROF_GR_ENGINE_ACTIVE,   gauge, Ratio of time the graphics engine is active (in %).
    # DCGM_FI_PROF_SM_ACTIVE,          gauge, The ratio of cycles an SM has at least 1 warp assigned (in %).
    # DCGM_FI_PROF_SM_OCCUPANCY,       gauge, The ratio of number of warps resident on an SM (in %).
    DCGM_FI_PROF_PIPE_TENSOR_ACTIVE, gauge, Ratio of cycles the tensor (HMMA) pipe is active (in %).
    DCGM_FI_PROF_DRAM_ACTIVE,        gauge, Ratio of cycles the device memory interface is active sending or receiving data (in %).
    # DCGM_FI_PROF_PIPE_FP64_ACTIVE,   gauge, Ratio of cycles the fp64 pipes are active (in %).
    # DCGM_FI_PROF_PIPE_FP32_ACTIVE,   gauge, Ratio of cycles the fp32 pipes are active (in %).
    # DCGM_FI_PROF_PIPE_FP16_ACTIVE,   gauge, Ratio of cycles the fp16 pipes are active (in %).
    DCGM_FI_PROF_PCIE_TX_BYTES,      gauge, The rate of data transmitted over the PCIe bus - including both protocol headers and data payloads - in bytes per second.
    DCGM_FI_PROF_PCIE_RX_BYTES,      gauge, The rate of data received over the PCIe bus - including both protocol headers and data payloads - in bytes per second.
    
    # Datadog additional recommended    fields
    DCGM_FI_DEV_COUNT,                  counter, Number of Devices on the node.
    DCGM_FI_DEV_FAN_SPEED,              gauge,   Fan speed for the device in percent 0-100.
    DCGM_FI_DEV_SLOWDOWN_TEMP,          gauge,   Slowdown temperature for the device.
    DCGM_FI_DEV_POWER_MGMT_LIMIT,       gauge,   Current power limit for the device.
    DCGM_FI_DEV_PSTATE,                 gauge,   Performance state (P-State) 0-15. 0=highest
    DCGM_FI_DEV_FB_TOTAL,               gauge,
    DCGM_FI_DEV_FB_RESERVED,            gauge,
    DCGM_FI_DEV_FB_USED_PERCENT,        gauge,
    DCGM_FI_DEV_CLOCK_THROTTLE_REASONS, gauge,  Current clock throttle reasons (bitmask of DCGM_CLOCKS_THROTTLE_REASON_*)
    
    DCGM_FI_PROCESS_NAME,               label,  The Process Name.
    DCGM_FI_CUDA_DRIVER_VERSION,        label,
    DCGM_FI_DEV_NAME,                   label,
    DCGM_FI_DEV_MINOR_NUMBER,           label,
    DCGM_FI_DRIVER_VERSION,             label,
    DCGM_FI_DEV_BRAND,                  label,
    DCGM_FI_DEV_SERIAL,                 label,
---
# Source: gpu-operator/charts/node-feature-discovery/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: gpu-operator-node-feature-discovery
  labels:
    helm.sh/chart: node-feature-discovery-0.14.2
    app.kubernetes.io/name: node-feature-discovery
    app.kubernetes.io/instance: gpu-operator
    app.kubernetes.io/version: "v0.14.2"
    app.kubernetes.io/managed-by: Helm
rules:
- apiGroups:
  - ""
  resources:
  - nodes
  - nodes/status
  verbs:
  - get
  - patch
  - update
  - list
- apiGroups:
  - nfd.k8s-sigs.io
  resources:
  - nodefeatures
  - nodefeaturerules
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - coordination.k8s.io
  resources:
  - leases
  verbs:
  - create
- apiGroups:
  - coordination.k8s.io
  resources:
  - leases
  resourceNames:
  - "nfd-master.nfd.kubernetes.io"
  verbs:
  - get
  - update
---
# Source: gpu-operator/charts/node-feature-discovery/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: gpu-operator-node-feature-discovery-gc
  labels:
    helm.sh/chart: node-feature-discovery-0.14.2
    app.kubernetes.io/name: node-feature-discovery
    app.kubernetes.io/instance: gpu-operator
    app.kubernetes.io/version: "v0.14.2"
    app.kubernetes.io/managed-by: Helm
rules:
- apiGroups:
  - ""
  resources:
  - nodes
  verbs:
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - nodes/proxy
  verbs:
  - get
- apiGroups:
  - topology.node.k8s.io
  resources:
  - noderesourcetopologies
  verbs:
  - delete
  - list
- apiGroups:
  - nfd.k8s-sigs.io
  resources:
  - nodefeatures
  verbs:
  - delete
  - list
---
# Source: gpu-operator/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: gpu-operator
  labels:
    app.kubernetes.io/name: gpu-operator
    helm.sh/chart: gpu-operator-v23.9.0
    app.kubernetes.io/instance: gpu-operator
    app.kubernetes.io/version: "v23.9.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: "gpu-operator"
rules:
- apiGroups:
  - config.openshift.io
  resources:
  - proxies
  verbs:
  - get
- apiGroups:
  - rbac.authorization.k8s.io
  resources:
  - roles
  - rolebindings
  - clusterroles
  - clusterrolebindings
  verbs:
  - '*'
- apiGroups:
  - ""
  resources:
  - pods
  - services
  - endpoints
  - persistentvolumeclaims
  - events
  - configmaps
  - secrets
  - serviceaccounts
  - nodes
  verbs:
  - '*'
- apiGroups:
  - ""
  resources:
  - namespaces
  verbs:
  - get
  - list
  - create
  - watch
  - update
  - patch
- apiGroups:
  - apps
  resources:
  - deployments
  - daemonsets
  - replicasets
  - statefulsets
  verbs:
  - '*'
- apiGroups:
  - apps
  resources:
  - controllerrevisions
  verbs:
  - 'get'
  - 'list'
  - 'watch'
- apiGroups:
  - monitoring.coreos.com
  resources:
  - servicemonitors
  - prometheusrules
  verbs:
  - get
  - list
  - create
  - watch
  - update
  - delete
- apiGroups:
  - nvidia.com
  resources:
  - '*'
  verbs:
  - '*'
- apiGroups:
  - scheduling.k8s.io
  resources:
  - priorityclasses
  verbs:
  - get
  - list
  - watch
  - create
- apiGroups:
  - security.openshift.io
  resources:
  - securitycontextconstraints
  verbs:
  - '*'
- apiGroups:
  - policy
  resources:
  - podsecuritypolicies
  verbs:
  - use
  resourceNames:
  - gpu-operator-restricted
- apiGroups:
  - policy
  resources:
  - podsecuritypolicies
  verbs:
  - create
  - get
  - update
  - list
  - delete
- apiGroups:
  - config.openshift.io
  resources:
  - clusterversions
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  - coordination.k8s.io
  resources:
  - configmaps
  - leases
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - patch
  - delete
- apiGroups:
  - node.k8s.io
  resources:
  - runtimeclasses
  verbs:
  - get
  - list
  - create
  - update
  - watch
  - delete
- apiGroups:
  - image.openshift.io
  resources:
  - imagestreams
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - pods
  - pods/eviction
  verbs:
  - get
  - list
  - watch
  - create
  - delete
  - update
  - patch
- apiGroups:
  - ""
  resources:
  - nodes
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - patch
- apiGroups:
  - apiextensions.k8s.io
  resources:
  - customresourcedefinitions
  verbs:
  - get
  - list
  - watch
  - update
  - patch
  - create
---
# Source: gpu-operator/charts/node-feature-discovery/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: gpu-operator-node-feature-discovery
  labels:
    helm.sh/chart: node-feature-discovery-0.14.2
    app.kubernetes.io/name: node-feature-discovery
    app.kubernetes.io/instance: gpu-operator
    app.kubernetes.io/version: "v0.14.2"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: gpu-operator-node-feature-discovery
subjects:
- kind: ServiceAccount
  name: node-feature-discovery
  namespace: gpu-operator
---
# Source: gpu-operator/charts/node-feature-discovery/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: gpu-operator-node-feature-discovery-gc
  labels:
    helm.sh/chart: node-feature-discovery-0.14.2
    app.kubernetes.io/name: node-feature-discovery
    app.kubernetes.io/instance: gpu-operator
    app.kubernetes.io/version: "v0.14.2"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: gpu-operator-node-feature-discovery-gc
subjects:
- kind: ServiceAccount
  name: node-feature-discovery
  namespace: gpu-operator
---
# Source: gpu-operator/templates/rolebinding.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: gpu-operator
  labels:
    app.kubernetes.io/name: gpu-operator
    helm.sh/chart: gpu-operator-v23.9.0
    app.kubernetes.io/instance: gpu-operator
    app.kubernetes.io/version: "v23.9.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: "gpu-operator"
subjects:
- kind: ServiceAccount
  name: gpu-operator
  namespace: gpu-operator
- kind: ServiceAccount
  name: node-feature-discovery
  namespace: gpu-operator
roleRef:
  kind: ClusterRole
  name: gpu-operator
  apiGroup: rbac.authorization.k8s.io
---
# Source: gpu-operator/charts/node-feature-discovery/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: gpu-operator-node-feature-discovery-worker
  namespace: gpu-operator
  labels:
    helm.sh/chart: node-feature-discovery-0.14.2
    app.kubernetes.io/name: node-feature-discovery
    app.kubernetes.io/instance: gpu-operator
    app.kubernetes.io/version: "v0.14.2"
    app.kubernetes.io/managed-by: Helm
rules:
- apiGroups:
  - nfd.k8s-sigs.io
  resources:
  - nodefeatures
  verbs:
  - create
  - get
  - update
---
# Source: gpu-operator/charts/node-feature-discovery/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: gpu-operator-node-feature-discovery-worker
  namespace: gpu-operator
  labels:
    helm.sh/chart: node-feature-discovery-0.14.2
    app.kubernetes.io/name: node-feature-discovery
    app.kubernetes.io/instance: gpu-operator
    app.kubernetes.io/version: "v0.14.2"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: gpu-operator-node-feature-discovery-worker
subjects:
- kind: ServiceAccount
  name: node-feature-discovery
  namespace: gpu-operator
---
# Source: gpu-operator/charts/node-feature-discovery/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: gpu-operator-node-feature-discovery-master
  namespace: gpu-operator
  labels:
    helm.sh/chart: node-feature-discovery-0.14.2
    app.kubernetes.io/name: node-feature-discovery
    app.kubernetes.io/instance: gpu-operator
    app.kubernetes.io/version: "v0.14.2"
    app.kubernetes.io/managed-by: Helm
    role: master
spec:
  type: ClusterIP
  ports:
    - port: 8080
      targetPort: grpc
      protocol: TCP
      name: grpc
  selector:
    app.kubernetes.io/name: node-feature-discovery
    app.kubernetes.io/instance: gpu-operator
    role: master
---
# Source: gpu-operator/charts/node-feature-discovery/templates/worker.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name:  gpu-operator-node-feature-discovery-worker
  namespace: gpu-operator
  labels:
    helm.sh/chart: node-feature-discovery-0.14.2
    app.kubernetes.io/name: node-feature-discovery
    app.kubernetes.io/instance: gpu-operator
    app.kubernetes.io/version: "v0.14.2"
    app.kubernetes.io/managed-by: Helm
    role: worker
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: node-feature-discovery
      app.kubernetes.io/instance: gpu-operator
      role: worker
  template:
    metadata:
      labels:
        app.kubernetes.io/name: node-feature-discovery
        app.kubernetes.io/instance: gpu-operator
        role: worker
    spec:
      dnsPolicy: ClusterFirstWithHostNet
      serviceAccountName: node-feature-discovery
      securityContext:
        {}
      containers:
      - name: worker
        securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
        image: "registry.k8s.io/nfd/node-feature-discovery:v0.14.2"
        imagePullPolicy: IfNotPresent
        env:
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        resources:
            {}
        command:
        - "nfd-worker"
        args:
        - "-server=gpu-operator-node-feature-discovery-master:8080"
        - "-metrics=8081"
        ports:
          - name: metrics
            containerPort: 8081
        volumeMounts:
        - name: host-boot
          mountPath: "/host-boot"
          readOnly: true
        - name: host-os-release
          mountPath: "/host-etc/os-release"
          readOnly: true
        - name: host-sys
          mountPath: "/host-sys"
          readOnly: true
        - name: host-usr-lib
          mountPath: "/host-usr/lib"
          readOnly: true
        - name: host-lib
          mountPath: "/host-lib"
          readOnly: true
        - name: source-d
          mountPath: "/etc/kubernetes/node-feature-discovery/source.d/"
          readOnly: true
        - name: features-d
          mountPath: "/etc/kubernetes/node-feature-discovery/features.d/"
          readOnly: true
        - name: nfd-worker-conf
          mountPath: "/etc/kubernetes/node-feature-discovery"
          readOnly: true
      volumes:
        - name: host-boot
          hostPath:
            path: "/boot"
        - name: host-os-release
          hostPath:
            path: "/etc/os-release"
        - name: host-sys
          hostPath:
            path: "/sys"
        - name: host-usr-lib
          hostPath:
            path: "/usr/lib"
        - name: host-lib
          hostPath:
            path: "/lib"
        - name: source-d
          hostPath:
            path: "/etc/kubernetes/node-feature-discovery/source.d/"
        - name: features-d
          hostPath:
            path: "/etc/kubernetes/node-feature-discovery/features.d/"
        - name: nfd-worker-conf
          configMap:
            name: gpu-operator-node-feature-discovery-worker-conf
            items:
              - key: nfd-worker.conf
                path: nfd-worker.conf
      tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Equal
          value: ""
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Equal
          value: ""
        - effect: NoSchedule
          key: nvidia.com/gpu
          operator: Exists
---
# Source: gpu-operator/charts/node-feature-discovery/templates/master.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name:  gpu-operator-node-feature-discovery-master
  namespace: gpu-operator
  labels:
    helm.sh/chart: node-feature-discovery-0.14.2
    app.kubernetes.io/name: node-feature-discovery
    app.kubernetes.io/instance: gpu-operator
    app.kubernetes.io/version: "v0.14.2"
    app.kubernetes.io/managed-by: Helm
    role: master
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: node-feature-discovery
      app.kubernetes.io/instance: gpu-operator
      role: master
  template:
    metadata:
      labels:
        app.kubernetes.io/name: node-feature-discovery
        app.kubernetes.io/instance: gpu-operator
        role: master
    spec:
      serviceAccountName: node-feature-discovery
      enableServiceLinks: false
      securityContext:
        {}
      containers:
        - name: master
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
          image: "registry.k8s.io/nfd/node-feature-discovery:v0.14.2"
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - "/usr/bin/grpc_health_probe"
              - "-addr=:8080"
            initialDelaySeconds: 10
            periodSeconds: 10
          readinessProbe:
            exec:
              command:
              - "/usr/bin/grpc_health_probe"
              - "-addr=:8080"
            initialDelaySeconds: 5
            periodSeconds: 10
            failureThreshold: 10
          ports:
          - containerPort: 8080
            name: grpc
          - containerPort: 8081
            name: metrics
          env:
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
          command:
            - "nfd-master"
          resources:
            {}
          args:
            - "-port=8080"
            ## By default, disable crd controller for other than the default instances
            - "-crd-controller=true"
            - "-metrics=8081"
          volumeMounts:
            - name: nfd-master-conf
              mountPath: "/etc/kubernetes/node-feature-discovery"
              readOnly: true
      volumes:
        - name: nfd-master-conf
          configMap:
            name: gpu-operator-node-feature-discovery-master-conf
            items:
              - key: nfd-master.conf
                path: nfd-master.conf
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - preference:
              matchExpressions:
              - key: node-role.kubernetes.io/master
                operator: In
                values:
                - ""
            weight: 1
          - preference:
              matchExpressions:
              - key: node-role.kubernetes.io/control-plane
                operator: In
                values:
                - ""
            weight: 1
      tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Equal
          value: ""
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Equal
          value: ""
---
# Source: gpu-operator/charts/node-feature-discovery/templates/nfd-gc.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gpu-operator-node-feature-discovery-gc
  namespace: gpu-operator
  labels:
    helm.sh/chart: node-feature-discovery-0.14.2
    app.kubernetes.io/name: node-feature-discovery
    app.kubernetes.io/instance: gpu-operator
    app.kubernetes.io/version: "v0.14.2"
    app.kubernetes.io/managed-by: Helm
    role: gc
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: node-feature-discovery
      app.kubernetes.io/instance: gpu-operator
      role: gc
  template:
    metadata:
      labels:
        app.kubernetes.io/name: node-feature-discovery
        app.kubernetes.io/instance: gpu-operator
        role: gc
    spec:
      serviceAccountName: node-feature-discovery
      dnsPolicy: ClusterFirstWithHostNet
      securityContext:
        {}
      containers:
      - name: gc
        image: "registry.k8s.io/nfd/node-feature-discovery:v0.14.2"
        imagePullPolicy: "IfNotPresent"
        env:
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        command:
          - "nfd-gc"
        args:
          - "-gc-interval=1h"
        resources:
            {}
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop: [ "ALL" ]
          readOnlyRootFilesystem: true
          runAsNonRoot: true
---
# Source: gpu-operator/templates/operator.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gpu-operator
  namespace: gpu-operator
  labels:
    app.kubernetes.io/name: gpu-operator
    helm.sh/chart: gpu-operator-v23.9.0
    app.kubernetes.io/instance: gpu-operator
    app.kubernetes.io/version: "v23.9.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: "gpu-operator"
    nvidia.com/gpu-driver-upgrade-drain.skip: "true"
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: "gpu-operator"
      app: "gpu-operator"
  template:
    metadata:
      labels:
        app.kubernetes.io/name: gpu-operator
        helm.sh/chart: gpu-operator-v23.9.0
        app.kubernetes.io/instance: gpu-operator
        app.kubernetes.io/version: "v23.9.0"
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: "gpu-operator"
        app: "gpu-operator"
        nvidia.com/gpu-driver-upgrade-drain.skip: "true"
      annotations:
        openshift.io/scc: restricted-readonly
    spec:
      serviceAccountName: gpu-operator
      priorityClassName: system-node-critical
      containers:
      - name: gpu-operator
        image: nvcr.io/nvidia/gpu-operator:v23.9.0
        imagePullPolicy: IfNotPresent
        command: ["gpu-operator"]
        args:
        - --leader-elect
        - --zap-time-encoding=epoch
        - --zap-log-level=info
        env:
        - name: WATCH_NAMESPACE
          value: ""
        - name: OPERATOR_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: "DRIVER_MANAGER_IMAGE"
          value: "nvcr.io/nvidia/cloud-native/k8s-driver-manager:v0.6.2"
        volumeMounts:
          - name: host-os-release
            mountPath: "/host-etc/os-release"
            readOnly: true
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8081
          initialDelaySeconds: 15
          periodSeconds: 20
        readinessProbe:
          httpGet:
            path: /readyz
            port: 8081
          initialDelaySeconds: 5
          periodSeconds: 10
        resources:
          limits:
            cpu: 500m
            memory: 350Mi
          requests:
            cpu: 200m
            memory: 100Mi
        ports:
          - name: metrics
            containerPort: 8080
      volumes:
        - name: host-os-release
          hostPath:
            path: "/etc/os-release"
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - preference:
              matchExpressions:
              - key: node-role.kubernetes.io/master
                operator: In
                values:
                - ""
            weight: 1
          - preference:
              matchExpressions:
              - key: node-role.kubernetes.io/control-plane
                operator: In
                values:
                - ""
            weight: 1
      tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Equal
          value: ""
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Equal
          value: ""
---
# Source: gpu-operator/templates/clusterpolicy.yaml
apiVersion: nvidia.com/v1
kind: ClusterPolicy
metadata:
  name: cluster-policy
  labels:
    app.kubernetes.io/name: gpu-operator
    helm.sh/chart: gpu-operator-v23.9.0
    app.kubernetes.io/instance: gpu-operator
    app.kubernetes.io/version: "v23.9.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: "gpu-operator"
spec:
  operator:
    defaultRuntime: docker
    runtimeClass: nvidia
    initContainer:
      repository: nvcr.io/nvidia
      image: cuda
      version: "12.2.2-base-ubi8"
      imagePullPolicy: IfNotPresent
  daemonsets:
    labels:
      helm.sh/chart: gpu-operator-v23.9.0
      app.kubernetes.io/managed-by: gpu-operator
    tolerations: 
      - effect: NoSchedule
        key: nvidia.com/gpu
        operator: Exists
    priorityClassName: system-node-critical
    updateStrategy: RollingUpdate
    rollingUpdate:
      maxUnavailable: "1"
  validator:
    repository: nvcr.io/nvidia/cloud-native
    image: gpu-operator-validator
    version: "v23.9.0"
    imagePullPolicy: IfNotPresent
    plugin:
      env: 
        - name: WITH_WORKLOAD
          value: "false"

  mig:
    strategy: single
  psp:
    enabled: false
  psa:
    enabled: false
  cdi:
    enabled: false
    default: false
  driver:
    enabled: false
    useNvidiaDriverCRD: false
    usePrecompiled: false
    repository: nvcr.io/nvidia
    image: driver
    version: "535.104.12"
    imagePullPolicy: IfNotPresent
    startupProbe: 
      failureThreshold: 120
      initialDelaySeconds: 60
      periodSeconds: 10
      timeoutSeconds: 60
    rdma:
      enabled: false
      useHostMofed: false
    manager:
      repository: nvcr.io/nvidia/cloud-native
      image: k8s-driver-manager
      version: "v0.6.4"
      imagePullPolicy: IfNotPresent
      env: 
        - name: ENABLE_GPU_POD_EVICTION
          value: "true"
        - name: ENABLE_AUTO_DRAIN
          value: "false"
        - name: DRAIN_USE_FORCE
          value: "false"
        - name: DRAIN_POD_SELECTOR_LABEL
          value: ""
        - name: DRAIN_TIMEOUT_SECONDS
          value: 0s
        - name: DRAIN_DELETE_EMPTYDIR_DATA
          value: "false"
    repoConfig: 
      configMapName: ""
    certConfig: 
      name: ""
    licensingConfig: 
      configMapName: ""
      nlsEnabled: true
    virtualTopology: 
      config: ""
    kernelModuleConfig: 
      name: ""
    upgradePolicy:
      autoUpgrade: true
      maxParallelUpgrades: 1
      maxUnavailable : 25%
      waitForCompletion:
        timeoutSeconds: 0
      podDeletion:
        force: false
        timeoutSeconds: 300
        deleteEmptyDir: false
      drain:
        enable: false
        force: false
        timeoutSeconds: 300
        deleteEmptyDir: false
  vgpuManager:
    enabled: false
    image: vgpu-manager
    imagePullPolicy: IfNotPresent
    driverManager:
      repository: nvcr.io/nvidia/cloud-native
      image: k8s-driver-manager
      version: "v0.6.4"
      imagePullPolicy: IfNotPresent
      env: 
        - name: ENABLE_GPU_POD_EVICTION
          value: "false"
        - name: ENABLE_AUTO_DRAIN
          value: "false"
  kataManager:
    enabled: false
    config: 
      artifactsDir: /opt/nvidia-gpu-operator/artifacts/runtimeclasses
      runtimeClasses:
      - artifacts:
          pullSecret: ""
          url: nvcr.io/nvidia/cloud-native/kata-gpu-artifacts:ubuntu22.04-535.54.03
        name: kata-qemu-nvidia-gpu
        nodeSelector: {}
      - artifacts:
          pullSecret: ""
          url: nvcr.io/nvidia/cloud-native/kata-gpu-artifacts:ubuntu22.04-535.86.10-snp
        name: kata-qemu-nvidia-gpu-snp
        nodeSelector:
          nvidia.com/cc.capable: "true"
    repository: nvcr.io/nvidia/cloud-native
    image: k8s-kata-manager
    version: "v0.1.2"
    imagePullPolicy: IfNotPresent
  vfioManager:
    enabled: true
    repository: nvcr.io/nvidia
    image: cuda
    version: "12.2.2-base-ubi8"
    imagePullPolicy: IfNotPresent
    driverManager:
      repository: nvcr.io/nvidia/cloud-native
      image: k8s-driver-manager
      version: "v0.6.2"
      imagePullPolicy: IfNotPresent
      env: 
        - name: ENABLE_GPU_POD_EVICTION
          value: "false"
        - name: ENABLE_AUTO_DRAIN
          value: "false"
  vgpuDeviceManager:
    enabled: true
    repository: nvcr.io/nvidia/cloud-native
    image: vgpu-device-manager
    version: "v0.2.4"
    imagePullPolicy: IfNotPresent
    config: 
      default: default
      name: ""
  ccManager:
    enabled: false
    defaultMode: "off"
    repository: nvcr.io/nvidia/cloud-native
    image: k8s-cc-manager
    version: "v0.1.1"
    imagePullPolicy: IfNotPresent
    env: 
      []
  toolkit:
    enabled: true
    repository: nvcr.io/nvidia/k8s
    image: container-toolkit
    version: "v1.14.3-ubuntu20.04"
    imagePullPolicy: IfNotPresent
    installDir: /usr/local/nvidia
  devicePlugin:
    enabled: true
    repository: nvcr.io/nvidia
    image: k8s-device-plugin
    version: "v0.14.2-ubi8"
    imagePullPolicy: IfNotPresent
    env: 
      - name: PASS_DEVICE_SPECS
        value: "true"
      - name: FAIL_ON_INIT_ERROR
        value: "true"
      - name: DEVICE_LIST_STRATEGY
        value: envvar
      - name: DEVICE_ID_STRATEGY
        value: uuid
      - name: NVIDIA_VISIBLE_DEVICES
        value: all
      - name: NVIDIA_DRIVER_CAPABILITIES
        value: all
  dcgm:
    enabled: false
    repository: nvcr.io/nvidia/cloud-native
    image: dcgm
    version: "3.2.6-1-ubuntu20.04"
    imagePullPolicy: IfNotPresent
    hostPort: 5555
  dcgmExporter:
    enabled: true
    repository: nvcr.io/nvidia/k8s
    image: dcgm-exporter
    version: "3.3.0-3.2.0-ubuntu22.04"
    imagePullPolicy: IfNotPresent
    env: 
      - name: DCGM_EXPORTER_LISTEN
        value: :9400
      - name: DCGM_EXPORTER_KUBERNETES
        value: "true"
      - name: DCGM_EXPORTER_COLLECTORS
        value: /etc/dcgm-exporter/dcgm-metrics.csv
    config: 
      name: metrics-config
    serviceMonitor: 
      additionalLabels: {}
      enabled: false
      honorLabels: false
      interval: 15s
      relabelings: []
  gfd:
    enabled: true
    repository: nvcr.io/nvidia
    image: gpu-feature-discovery
    version: "v0.8.2-ubi8"
    imagePullPolicy: IfNotPresent
    env: 
      - name: GFD_SLEEP_INTERVAL
        value: 60s
      - name: GFD_FAIL_ON_INIT_ERROR
        value: "true"
  migManager:
    enabled: true
    repository: nvcr.io/nvidia/cloud-native
    image: k8s-mig-manager
    version: "v0.5.5-ubuntu20.04"
    imagePullPolicy: IfNotPresent
    env: 
      - name: WITH_REBOOT
        value: "false"
    config: 
      default: all-disabled
      name: default-mig-parted-config
    gpuClientsConfig: 
      name: ""
  nodeStatusExporter:
    enabled: false
    repository: nvcr.io/nvidia/cloud-native
    image: gpu-operator-validator
    version: "v23.9.0"
    imagePullPolicy: IfNotPresent
  sandboxWorkloads:
    enabled: false
    defaultWorkload: container
  sandboxDevicePlugin:
    enabled: true
    repository: nvcr.io/nvidia
    image: kubevirt-gpu-device-plugin
    version: "v1.2.3"
    imagePullPolicy: IfNotPresent

