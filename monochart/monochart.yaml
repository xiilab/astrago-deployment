---
# Source: uyuni/charts/mariadb/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: mariadb
  namespace: "uyuni"
  labels:
    app.kubernetes.io/name: mariadb
    helm.sh/chart: mariadb-12.2.9
    app.kubernetes.io/instance: uyuni
    app.kubernetes.io/managed-by: Helm
  annotations:
automountServiceAccountToken: false
---
# Source: uyuni/charts/mariadb/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: mariadb
  namespace: "uyuni"
  labels:
    app.kubernetes.io/name: mariadb
    helm.sh/chart: mariadb-12.2.9
    app.kubernetes.io/instance: uyuni
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
  mariadb-root-password: "cm9vdA=="
  mariadb-password: "eGlpcm9ja3M="
---
# Source: uyuni/charts/mariadb/templates/primary/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: mariadb
  namespace: "uyuni"
  labels:
    app.kubernetes.io/name: mariadb
    helm.sh/chart: mariadb-12.2.9
    app.kubernetes.io/instance: uyuni
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: primary
data:
  my.cnf: |-
    [mysqld]
    skip-name-resolve
    explicit_defaults_for_timestamp
    basedir=/opt/bitnami/mariadb
    plugin_dir=/opt/bitnami/mariadb/plugin
    port=3306
    socket=/opt/bitnami/mariadb/tmp/mysql.sock
    tmpdir=/opt/bitnami/mariadb/tmp
    max_allowed_packet=16M
    bind-address=*
    pid-file=/opt/bitnami/mariadb/tmp/mysqld.pid
    log-error=/opt/bitnami/mariadb/logs/mysqld.log
    character-set-server=UTF8
    collation-server=utf8_general_ci
    slow_query_log=0
    slow_query_log_file=/opt/bitnami/mariadb/logs/mysqld.log
    long_query_time=10.0
    interactive_timeout=1803
    wait_timeout=1803
    
    [client]
    port=3306
    socket=/opt/bitnami/mariadb/tmp/mysql.sock
    default-character-set=UTF8
    plugin_dir=/opt/bitnami/mariadb/plugin
    
    [manager]
    port=3306
    socket=/opt/bitnami/mariadb/tmp/mysql.sock
    pid-file=/opt/bitnami/mariadb/tmp/mysqld.pid
---
# Source: uyuni/templates/storage-class.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-csi
  namespace: uyuni
provisioner: nfs.csi.k8s.io
parameters:
  server: 10.61.3.2
  share: /kube-storage
  # csi.storage.k8s.io/provisioner-secret is only needed for providing mountOptions in DeleteVolume
  # csi.storage.k8s.io/provisioner-secret-name: "mount-options"
  # csi.storage.k8s.io/provisioner-secret-namespace: "default"
reclaimPolicy: Delete
volumeBindingMode: Immediate
mountOptions:
  - nfsvers=4.1
---
# Source: uyuni/charts/batch/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: uyuni-backend-batch
  namespace: 
  labels:
    helm.sh/chart: batch-0.1.1
    app.kubernetes.io/name: batch
    app.kubernetes.io/instance: uyuni
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  ports:
    - port: 8080
      targetPort: 8080
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: batch
    app.kubernetes.io/instance: uyuni
---
# Source: uyuni/charts/core/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: uyuni-backend-core
  namespace: 
  labels:
    helm.sh/chart: core-0.1.1
    app.kubernetes.io/name: core
    app.kubernetes.io/instance: uyuni
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  ports:
    - port: 8080
      targetPort: 8080
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: core
    app.kubernetes.io/instance: uyuni
---
# Source: uyuni/charts/frontend/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: uyuni-frontend
  namespace: 
  labels:
    helm.sh/chart: frontend-0.1.0
    app.kubernetes.io/name: frontend
    app.kubernetes.io/instance: uyuni
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: NodePort
  ports:
    - port: 3000
      targetPort: 3000
      nodePort: 30080
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: frontend
    app.kubernetes.io/instance: uyuni
---
# Source: uyuni/charts/mariadb/templates/primary/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: mariadb
  namespace: "uyuni"
  labels:
    app.kubernetes.io/name: mariadb
    helm.sh/chart: mariadb-12.2.9
    app.kubernetes.io/instance: uyuni
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: primary
  annotations:
spec:
  type: NodePort
  externalTrafficPolicy: "Cluster"
  sessionAffinity: None
  ports:
    - name: mysql
      port: 3306
      protocol: TCP
      targetPort: mysql
      nodePort: 32000
  selector: 
    app.kubernetes.io/name: mariadb
    app.kubernetes.io/instance: uyuni
    app.kubernetes.io/component: primary
---
# Source: uyuni/charts/monitor/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: uyuni-backend-monitor
  namespace: 
  labels:
    helm.sh/chart: monitor-0.1.1
    app.kubernetes.io/name: monitor
    app.kubernetes.io/instance: uyuni
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  ports:
    - port: 8080
      targetPort: 8080
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: monitor
    app.kubernetes.io/instance: uyuni
---
# Source: uyuni/charts/batch/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: uyuni-backend-batch
  namespace: 
  labels:
    helm.sh/chart: batch-0.1.1
    app.kubernetes.io/name: batch
    app.kubernetes.io/instance: uyuni
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: batch
      app.kubernetes.io/instance: uyuni
  template:
    metadata:
      labels:
        app.kubernetes.io/name: batch
        app.kubernetes.io/instance: uyuni
    spec:
      securityContext:
        {}
      containers:
        - name: batch
          securityContext:
            {}
          image: "harbor.xiilab.com:32443/uyuni/server-batch:develop-79a0021"
          imagePullPolicy: IfNotPresent

          env:
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
          resources:
            {}
---
# Source: uyuni/charts/core/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: uyuni-backend-core
  namespace: 
  labels:
    helm.sh/chart: core-0.1.1
    app.kubernetes.io/name: core
    app.kubernetes.io/instance: uyuni
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: core
      app.kubernetes.io/instance: uyuni
  template:
    metadata:
      labels:
        app.kubernetes.io/name: core
        app.kubernetes.io/instance: uyuni
    spec:
      securityContext:
        {}
      containers:
        - name: core
          securityContext:
            {}
          image: "harbor.xiilab.com:32443/uyuni/server-core:develop-79a0021"
          imagePullPolicy: IfNotPresent
          
          env:

          
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP

          resources:
            {}
---
# Source: uyuni/charts/frontend/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: uyuni-frontend
  namespace: 
  labels:
    helm.sh/chart: frontend-0.1.0
    app.kubernetes.io/name: frontend
    app.kubernetes.io/instance: uyuni
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: frontend
      app.kubernetes.io/instance: uyuni
  template:
    metadata:
      labels:
        app.kubernetes.io/name: frontend
        app.kubernetes.io/instance: uyuni
    spec:
      securityContext:
        {}
      containers:
        - name: frontend
          securityContext:
            {}
          image: "harbor.xiilab.com:32443/uyuni/uyuni-frontend:develop-6fac3da"
          imagePullPolicy: Always
          
          env:
          - name: KEYCLOAK_HOST
            value: "https://10.61.3.8:30002"
          - name: KEYCLOAK_REALME
            value: "myrealm"
          - name: KEYCLOAK_CLIENT_ID
            value: "kubernetes-client"
          - name: AUTH_CLIENT_SECRET
            value: "7bE2Oq2HyKrPsX49EXul0G48O4c4kkFv"
          - name: NEXTAUTH_URL
            value: "http://localhost:3000"
          - name: NEXTAUTH_SECRET
            value: "uuNj1L0Yg2xKcBPVp7yOVlm2nigL3hoHOzbwQXAwx1I="
          - name: NEXT_PUBLIC_API_URL
            value: "http://localhost:4444/"
          
          ports:
            - name: http
              containerPort: 3000
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /
              port: http
          readinessProbe:
            httpGet:
              path: /
              port: http
          resources:
            {}
---
# Source: uyuni/charts/monitor/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: uyuni-backend-monitor
  namespace: 
  labels:
    helm.sh/chart: monitor-0.1.1
    app.kubernetes.io/name: monitor
    app.kubernetes.io/instance: uyuni
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: monitor
      app.kubernetes.io/instance: uyuni
  template:
    metadata:
      labels:
        app.kubernetes.io/name: monitor
        app.kubernetes.io/instance: uyuni
    spec:
      securityContext:
        {}
      containers:
        - name: monitor
          securityContext:
            {}
          image: "harbor.xiilab.com:32443/uyuni/server-monitor:develop-79a0021"
          imagePullPolicy: IfNotPresent
          env:
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
          resources:
            {}
---
# Source: uyuni/charts/mariadb/templates/primary/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mariadb
  namespace: "uyuni"
  labels:
    app.kubernetes.io/name: mariadb
    helm.sh/chart: mariadb-12.2.9
    app.kubernetes.io/instance: uyuni
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: primary
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels: 
      app.kubernetes.io/name: mariadb
      app.kubernetes.io/instance: uyuni
      app.kubernetes.io/component: primary
  serviceName: mariadb
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      annotations:
        checksum/configuration: c4b338e2e83abbf552a0a633bc45a2f3ec80d7230ba0583e01198dfa4a40efb9
      labels:
        app.kubernetes.io/name: mariadb
        helm.sh/chart: mariadb-12.2.9
        app.kubernetes.io/instance: uyuni
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: primary
    spec:
      
      serviceAccountName: mariadb
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: mariadb
                    app.kubernetes.io/instance: uyuni
                    app.kubernetes.io/component: primary
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      nodeSelector:
        node-role.kubernetes.io/control-plane: ""
      tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
      securityContext:
        fsGroup: 1001
      containers:
        - name: mariadb
          image: docker.io/bitnami/mariadb:10.11.4-debian-11-r46
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            privileged: false
            runAsNonRoot: true
            runAsUser: 1001
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MARIADB_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: mariadb
                  key: mariadb-root-password
            - name: MARIADB_USER
              value: "uyuni"
            - name: MARIADB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: mariadb
                  key: mariadb-password
            - name: MARIADB_DATABASE
              value: "uyuni"
          ports:
            - name: mysql
              containerPort: 3306
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 120
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
            exec:
              command:
                - /bin/bash
                - -ec
                - |
                  password_aux="${MARIADB_ROOT_PASSWORD:-}"
                  if [[ -f "${MARIADB_ROOT_PASSWORD_FILE:-}" ]]; then
                      password_aux=$(cat "$MARIADB_ROOT_PASSWORD_FILE")
                  fi
                  mysqladmin status -uroot -p"${password_aux}"
          readinessProbe:
            failureThreshold: 3
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
            exec:
              command:
                - /bin/bash
                - -ec
                - |
                  password_aux="${MARIADB_ROOT_PASSWORD:-}"
                  if [[ -f "${MARIADB_ROOT_PASSWORD_FILE:-}" ]]; then
                      password_aux=$(cat "$MARIADB_ROOT_PASSWORD_FILE")
                  fi
                  mysqladmin status -uroot -p"${password_aux}"
          resources: 
            limits: {}
            requests: {}
          volumeMounts:
            - name: data
              mountPath: /bitnami/mariadb
            - name: config
              mountPath: /opt/bitnami/mariadb/conf/my.cnf
              subPath: my.cnf
      volumes:
        - name: config
          configMap:
            name: mariadb
  volumeClaimTemplates:
    - metadata:
        name: data
        labels: 
          app.kubernetes.io/name: mariadb
          app.kubernetes.io/instance: uyuni
          app.kubernetes.io/component: primary
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
        storageClassName: nfs-csi

---
# Source: csi-driver-nfs/templates/rbac-csi-nfs.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: csi-nfs-controller-sa
  namespace: kube-system
  labels:
    app.kubernetes.io/instance: "nfs-provisioner"
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/name: "csi-driver-nfs"
    app.kubernetes.io/version: "v4.5.0"
    helm.sh/chart: "csi-driver-nfs-v4.5.0"
---
# Source: csi-driver-nfs/templates/rbac-csi-nfs.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: csi-nfs-node-sa
  namespace: kube-system
  labels:
    app.kubernetes.io/instance: "nfs-provisioner"
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/name: "csi-driver-nfs"
    app.kubernetes.io/version: "v4.5.0"
    helm.sh/chart: "csi-driver-nfs-v4.5.0"
---
# Source: csi-driver-nfs/templates/rbac-csi-nfs.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nfs-external-provisioner-role
  labels:
    app.kubernetes.io/instance: "nfs-provisioner"
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/name: "csi-driver-nfs"
    app.kubernetes.io/version: "v4.5.0"
    helm.sh/chart: "csi-driver-nfs-v4.5.0"
rules:
  - apiGroups: [""]
    resources: ["persistentvolumes"]
    verbs: ["get", "list", "watch", "create", "delete"]
  - apiGroups: [""]
    resources: ["persistentvolumeclaims"]
    verbs: ["get", "list", "watch", "update"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["get", "list", "watch", "create", "update", "patch"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["csinodes"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["coordination.k8s.io"]
    resources: ["leases"]
    verbs: ["get", "list", "watch", "create", "update", "patch"]
  - apiGroups: [""]
    resources: ["secrets"]
    verbs: ["get"]
---
# Source: csi-driver-nfs/templates/rbac-csi-nfs.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nfs-csi-provisioner-binding
  labels:
    app.kubernetes.io/instance: "nfs-provisioner"
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/name: "csi-driver-nfs"
    app.kubernetes.io/version: "v4.5.0"
    helm.sh/chart: "csi-driver-nfs-v4.5.0"
subjects:
  - kind: ServiceAccount
    name: csi-nfs-controller-sa
    namespace: kube-system
roleRef:
  kind: ClusterRole
  name: nfs-external-provisioner-role
  apiGroup: rbac.authorization.k8s.io
---
# Source: csi-driver-nfs/templates/csi-nfs-node.yaml
kind: DaemonSet
apiVersion: apps/v1
metadata:
  name: csi-nfs-node
  namespace: kube-system
  labels:
    app.kubernetes.io/instance: "nfs-provisioner"
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/name: "csi-driver-nfs"
    app.kubernetes.io/version: "v4.5.0"
    helm.sh/chart: "csi-driver-nfs-v4.5.0"
spec:
  updateStrategy:
    rollingUpdate:
      maxUnavailable: 1
    type: RollingUpdate
  selector:
    matchLabels:
      app: csi-nfs-node
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: "nfs-provisioner"
        app.kubernetes.io/managed-by: "Helm"
        app.kubernetes.io/name: "csi-driver-nfs"
        app.kubernetes.io/version: "v4.5.0"
        helm.sh/chart: "csi-driver-nfs-v4.5.0"
        app: csi-nfs-node
    spec:
      hostNetwork: true # original nfs connection would be broken without hostNetwork setting
      dnsPolicy: ClusterFirstWithHostNet
      serviceAccountName: csi-nfs-node-sa
      priorityClassName: system-cluster-critical
      securityContext:
        seccompProfile:
          type: RuntimeDefault
      nodeSelector:
        kubernetes.io/os: linux
      tolerations:
        - operator: Exists
      containers:
        - name: liveness-probe
          image: "registry.k8s.io/sig-storage/livenessprobe:v2.11.0"
          args:
            - --csi-address=/csi/csi.sock
            - --probe-timeout=3s
            - --health-port=29653
            - --v=2
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - name: socket-dir
              mountPath: /csi
          resources:
            limits:
              memory: 100Mi
            requests:
              cpu: 10m
              memory: 20Mi
          securityContext:
            readOnlyRootFilesystem: true
        - name: node-driver-registrar
          image: "registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.9.0"
          livenessProbe:
            exec:
              command:
                - /csi-node-driver-registrar
                - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)
                - --mode=kubelet-registration-probe
            initialDelaySeconds: 30
            timeoutSeconds: 15
          args:
            - --v=2
            - --csi-address=/csi/csi.sock
            - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)
          env:
            - name: DRIVER_REG_SOCK_PATH
              value: /var/lib/kubelet/plugins/csi-nfsplugin/csi.sock
            - name: KUBE_NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - name: socket-dir
              mountPath: /csi
            - name: registration-dir
              mountPath: /registration
          resources:
            limits:
              memory: 100Mi
            requests:
              cpu: 10m
              memory: 20Mi
        - name: nfs
          securityContext:
            privileged: true
            capabilities:
              add: ["SYS_ADMIN"]
            allowPrivilegeEscalation: true
            readOnlyRootFilesystem: true
          image: "registry.k8s.io/sig-storage/nfsplugin:v4.5.0"
          args :
            - "--v=5"
            - "--nodeid=$(NODE_ID)"
            - "--endpoint=$(CSI_ENDPOINT)"
            - "--drivername=nfs.csi.k8s.io"
            - "--mount-permissions=0"
          env:
            - name: NODE_ID
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: CSI_ENDPOINT
              value: unix:///csi/csi.sock
          ports:
            - containerPort: 29653
              name: healthz
              protocol: TCP
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /healthz
              port: healthz
            initialDelaySeconds: 30
            timeoutSeconds: 10
            periodSeconds: 30
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - name: socket-dir
              mountPath: /csi
            - name: pods-mount-dir
              mountPath: /var/lib/kubelet/pods
              mountPropagation: "Bidirectional"
          resources:
            limits:
              memory: 300Mi
            requests:
              cpu: 10m
              memory: 20Mi
      volumes:
        - name: socket-dir
          hostPath:
            path: /var/lib/kubelet/plugins/csi-nfsplugin
            type: DirectoryOrCreate
        - name: pods-mount-dir
          hostPath:
            path: /var/lib/kubelet/pods
            type: Directory
        - hostPath:
            path: /var/lib/kubelet/plugins_registry
            type: Directory
          name: registration-dir
---
# Source: csi-driver-nfs/templates/csi-nfs-controller.yaml
kind: Deployment
apiVersion: apps/v1
metadata:
  name: csi-nfs-controller
  namespace: kube-system
  labels:
    app.kubernetes.io/instance: "nfs-provisioner"
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/name: "csi-driver-nfs"
    app.kubernetes.io/version: "v4.5.0"
    helm.sh/chart: "csi-driver-nfs-v4.5.0"
spec:
  replicas: 1
  selector:
    matchLabels:
      app: csi-nfs-controller
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: "nfs-provisioner"
        app.kubernetes.io/managed-by: "Helm"
        app.kubernetes.io/name: "csi-driver-nfs"
        app.kubernetes.io/version: "v4.5.0"
        helm.sh/chart: "csi-driver-nfs-v4.5.0"
        app: csi-nfs-controller
    spec:
      hostNetwork: true  # controller also needs to mount nfs to create dir
      dnsPolicy: ClusterFirstWithHostNet
      serviceAccountName: csi-nfs-controller-sa
      nodeSelector:
        kubernetes.io/os: linux
      priorityClassName: system-cluster-critical
      securityContext:
        seccompProfile:
          type: RuntimeDefault
      tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/controlplane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
      containers:
        - name: csi-provisioner
          image: "registry.k8s.io/sig-storage/csi-provisioner:v3.6.1"
          args:
            - "-v=2"
            - "--csi-address=$(ADDRESS)"
            - "--leader-election"
            - "--leader-election-namespace=kube-system"
            - "--extra-create-metadata=true"
            - "--timeout=1200s"
          env:
            - name: ADDRESS
              value: /csi/csi.sock
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - mountPath: /csi
              name: socket-dir
          resources:
            limits:
              memory: 400Mi
            requests:
              cpu: 10m
              memory: 20Mi
          securityContext:
            readOnlyRootFilesystem: true
        - name: csi-snapshotter
          image: "registry.k8s.io/sig-storage/csi-snapshotter:v6.3.1"
          args:
            - "--v=2"
            - "--csi-address=$(ADDRESS)"
            - "--leader-election-namespace=kube-system"
            - "--leader-election"
            - "--timeout=1200s"
          env:
            - name: ADDRESS
              value: /csi/csi.sock
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              memory: 200Mi
            requests:
              cpu: 10m
              memory: 20Mi
          volumeMounts:
            - name: socket-dir
              mountPath: /csi
        - name: liveness-probe
          image: "registry.k8s.io/sig-storage/livenessprobe:v2.11.0"
          args:
            - --csi-address=/csi/csi.sock
            - --probe-timeout=3s
            - --health-port=29652
            - --v=2
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - name: socket-dir
              mountPath: /csi
          resources:
            limits:
              memory: 100Mi
            requests:
              cpu: 10m
              memory: 20Mi
          securityContext:
            readOnlyRootFilesystem: true
        - name: nfs
          image: "registry.k8s.io/sig-storage/nfsplugin:v4.5.0"
          securityContext:
            privileged: true
            capabilities:
              add: ["SYS_ADMIN"]
            allowPrivilegeEscalation: true
            readOnlyRootFilesystem: true
          imagePullPolicy: IfNotPresent
          args:
            - "--v=5"
            - "--nodeid=$(NODE_ID)"
            - "--endpoint=$(CSI_ENDPOINT)"
            - "--drivername=nfs.csi.k8s.io"
            - "--mount-permissions=0"
            - "--working-mount-dir=/tmp"
            - "--default-ondelete-policy=delete"
          env:
            - name: NODE_ID
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: CSI_ENDPOINT
              value: unix:///csi/csi.sock
          ports:
            - containerPort: 29652
              name: healthz
              protocol: TCP
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /healthz
              port: healthz
            initialDelaySeconds: 30
            timeoutSeconds: 10
            periodSeconds: 30
          volumeMounts:
            - name: pods-mount-dir
              mountPath: /var/lib/kubelet/pods
              mountPropagation: "Bidirectional"
            - mountPath: /csi
              name: socket-dir
            - mountPath: /tmp
              name: tmp-dir
          resources:
            limits:
              memory: 200Mi
            requests:
              cpu: 10m
              memory: 20Mi
      volumes:
        - name: pods-mount-dir
          hostPath:
            path: /var/lib/kubelet/pods
            type: Directory
        - name: socket-dir
          emptyDir: {}
        - name: tmp-dir
          emptyDir: {}
---
# Source: csi-driver-nfs/templates/csi-nfs-driverinfo.yaml
apiVersion: storage.k8s.io/v1
kind: CSIDriver
metadata:
  name: nfs.csi.k8s.io
spec:
  attachRequired: false
  volumeLifecycleModes:
    - Persistent
  fsGroupPolicy: File

