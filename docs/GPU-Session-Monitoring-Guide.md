# GPU ì„¸ì…˜ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ê°€ì´ë“œ

## ê°œìš”

GPU ì„¸ì…˜ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œì€ Kubernetes í´ëŸ¬ìŠ¤í„°ì—ì„œ GPU ë¦¬ì†ŒìŠ¤ë¥¼ ì‚¬ìš©í•˜ëŠ” í”„ë¡œì„¸ìŠ¤ì™€ Podë“¤ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¶”ì í•˜ê³  ëª¨ë‹ˆí„°ë§í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤. ì´ ì‹œìŠ¤í…œì€ Prometheusì™€ ì—°ë™í•˜ì—¬ GPU ì‚¬ìš©ëŸ‰, ë©”ëª¨ë¦¬ ì ìœ ìœ¨, Pod ì •ë³´ ë“±ì„ ìˆ˜ì§‘í•˜ê³  ì‹œê°í™”í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.

## ì£¼ìš” ê¸°ëŠ¥

### 1. ğŸ” ì‹¤ì‹œê°„ GPU í”„ë¡œì„¸ìŠ¤ ì¶”ì 
- **ì •í™•í•œ PID ë§¤í•‘**: nvidia-smiì™€ Prometheusì—ì„œ ë™ì¼í•œ PID ì •ë³´ ì œê³µ
- **GPUë³„ í”„ë¡œì„¸ìŠ¤ ë¶„ë¦¬**: ê° GPUì—ì„œ ì‹¤í–‰ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤ë¥¼ ì •í™•íˆ êµ¬ë¶„
- **Pod ì •ë³´ ì—°ê²°**: Kubernetes Podì™€ GPU í”„ë¡œì„¸ìŠ¤ë¥¼ ìë™ìœ¼ë¡œ ì—°ê²°

### 2. ğŸ¯ MIG(Multi-Instance GPU) ì§€ì›
- **ìë™ MIG ê°ì§€**: MIG ëª¨ë“œ í™œì„±í™” ì—¬ë¶€ë¥¼ ìë™ìœ¼ë¡œ íŒë‹¨
- **MIG ì¸ìŠ¤í„´ìŠ¤ë³„ ëª¨ë‹ˆí„°ë§**: ê° MIG ì¸ìŠ¤í„´ìŠ¤ì˜ ë…ë¦½ì ì¸ í”„ë¡œì„¸ìŠ¤ ì¶”ì 
- **í˜¸í™˜ì„± ë³´ì¥**: MIG ë¹„í™œì„±í™” í™˜ê²½ì—ì„œë„ ì •ìƒ ì‘ë™

### 3. ğŸ“Š ë‹¤ì–‘í•œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
- **í”„ë¡œì„¸ìŠ¤ ì •ë³´**: PID, ëª…ë ¹ì–´, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
- **GPU ì‚¬ìš©ëŸ‰**: GPUë³„ ì„¸ì…˜ ìˆ˜, ì´ ì„¸ì…˜ ìˆ˜
- **Pod ì—°ê²° ì •ë³´**: ë„¤ì„ìŠ¤í˜ì´ìŠ¤, Pod ì´ë¦„, ì»¨í…Œì´ë„ˆ ì •ë³´
- **ìƒíƒœ ëª¨ë‹ˆí„°ë§**: í™œì„±/ìœ íœ´ ìƒíƒœ êµ¬ë¶„

## ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CronJob       â”‚    â”‚  Node Exporter   â”‚    â”‚   Prometheus    â”‚
â”‚ (GPU Collector) â”‚â”€â”€â”€â–¶â”‚ (Textfile Dir)   â”‚â”€â”€â”€â–¶â”‚   (Scraping)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                               â”‚
         â–¼                                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   nvidia-smi    â”‚                              â”‚     Grafana     â”‚
â”‚   (GPU Info)    â”‚                              â”‚ (Visualization) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ì‘ë™ ì›ë¦¬

### 1. ë°ì´í„° ìˆ˜ì§‘ í”„ë¡œì„¸ìŠ¤

#### 1ë‹¨ê³„: MIG ëª¨ë“œ ê°ì§€
```bash
# MIG ëª¨ë“œ í™•ì¸
nvidia-smi --query-gpu=mig.mode.current --format=csv,noheader,nounits
```

#### 2ë‹¨ê³„: GPU ëª©ë¡ ë™ì  ìƒì„±
```bash
# MIG ë¹„í™œì„±í™”ì‹œ
GPU 0, GPU 1, GPU 2, GPU 3

# MIG í™œì„±í™”ì‹œ (ì˜ˆì‹œ)
MIG 0/0/0, MIG 0/1/0, GPU 1, GPU 2, GPU 3
```

#### 3ë‹¨ê³„: í”„ë¡œì„¸ìŠ¤ ì •ë³´ ìˆ˜ì§‘
```bash
# MIG í™˜ê²½
nvidia-smi --query-compute-apps=gpu_uuid,pid,process_name,used_gpu_memory

# ì¼ë°˜ í™˜ê²½  
nvidia-smi --query-compute-apps=gpu_bus_id,pid,process_name,used_gpu_memory
```

#### 4ë‹¨ê³„: Pod ì •ë³´ ë§¤í•‘
```bash
# /proc/{PID}/cgroupì—ì„œ Kubernetes Pod ì •ë³´ ì¶”ì¶œ
/proc/12345/cgroup â†’ pod-abc123-def456
```

### 2. ë©”íŠ¸ë¦­ ìƒì„±

ì‹œìŠ¤í…œì€ ë‹¤ìŒê³¼ ê°™ì€ Prometheus ë©”íŠ¸ë¦­ì„ ìƒì„±í•©ë‹ˆë‹¤:

#### GPU í”„ë¡œì„¸ìŠ¤ ì •ë³´
```prometheus
gpu_process_info{
  gpu="0",
  pid="12345", 
  command="python",
  pod="training-pod-abc123",
  namespace="ml-workspace",
  container="pytorch",
  status="active",
  gpu_memory="1024MiB"
} 1
```

#### GPU ì„¸ì…˜ ìˆ˜
```prometheus
gpu_session_count{gpu="0"} 2
gpu_session_count{gpu="1"} 0
gpu_total_sessions 2
```

#### í”„ë¡œì„¸ìŠ¤ ì‚¬ìš©ë¥ 
```prometheus
gpu_process_utilization{gpu="0",pid="12345",...} 85
gpu_process_memory_utilization{gpu="0",pid="12345",...} 60
```

## ì„¤ì • ë° ë°°í¬

### 1. ê¸°ë³¸ ì„¤ì • (values.yaml)
```yaml
gpu-session-monitoring:
  enabled: true

collection:
  schedule: "*/1 * * * *"  # 1ë¶„ë§ˆë‹¤ ì‹¤í–‰
  resources:
    requests:
      memory: "128Mi"
      cpu: "100m"
    limits:
      memory: "256Mi" 
      cpu: "200m"

nodeSelector:
  accelerator: "nvidia"

tolerations:
  - key: "nvidia.com/gpu"
    operator: "Exists"
    effect: "NoSchedule"
```

### 2. ë°°í¬ ëª…ë ¹ì–´
```bash
# Helmì„ ì‚¬ìš©í•œ ë°°í¬
helm upgrade --install gpu-monitoring . \
  --set gpu-session-monitoring.enabled=true \
  --namespace monitoring
```

## ëª¨ë‹ˆí„°ë§ ë° í™•ì¸

### 1. ìˆ˜ì§‘ ìƒíƒœ í™•ì¸
```bash
# CronJob ì‹¤í–‰ ìƒíƒœ í™•ì¸
kubectl get cronjob gpu-session-metrics-collector

# ìµœê·¼ ì‹¤í–‰ ë¡œê·¸ í™•ì¸
kubectl logs -l job-name=gpu-session-metrics-collector-<timestamp>
```

### 2. ë©”íŠ¸ë¦­ íŒŒì¼ í™•ì¸
```bash
# ë…¸ë“œì—ì„œ ì§ì ‘ í™•ì¸
cat /var/lib/node_exporter/textfile_collector/gpu_sessions.prom
```

### 3. Prometheusì—ì„œ ì¡°íšŒ
```prometheus
# GPUë³„ í˜„ì¬ ì„¸ì…˜ ìˆ˜
gpu_session_count

# íŠ¹ì • GPUì˜ í™œì„± í”„ë¡œì„¸ìŠ¤
gpu_process_info{gpu="0", status="active"}

# ì „ì²´ GPU ì„¸ì…˜ ìˆ˜
gpu_total_sessions
```

## MIG í™˜ê²½ì—ì„œì˜ ë™ì‘

### MIG ë¹„í™œì„±í™” í™˜ê²½
```
GPU ë¼ë²¨: gpu="0", gpu="1", gpu="2", gpu="3"
ì‹ë³„ ë°©ì‹: PCI Bus ID ê¸°ë°˜
ë§¤í•‘: nvidia-smi bus_id â†” GPU index
```

### MIG í™œì„±í™” í™˜ê²½
```
GPU ë¼ë²¨: gpu="0_0_0", gpu="0_1_0", gpu="1", gpu="2"
ì‹ë³„ ë°©ì‹: GPU UUID ê¸°ë°˜  
ë§¤í•‘: nvidia-smi uuid â†” MIG instance
```

### MIG ì„¤ì • ì˜ˆì‹œ
```bash
# MIG ëª¨ë“œ í™œì„±í™”
nvidia-smi -mig 1 -i 0

# MIG ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (1g.5gb í”„ë¡œí•„ 2ê°œ)
nvidia-smi mig -cgi 1g.5gb,1g.5gb -i 0

# ê²°ê³¼ í™•ì¸
nvidia-smi -L
# GPU 0: A100-PCIE-40GB (UUID: GPU-...)
#   MIG 1g.5gb     Device  0: (UUID: MIG-...)
#   MIG 1g.5gb     Device  1: (UUID: MIG-...)
```

## ë¬¸ì œ í•´ê²°

### 1. ë©”íŠ¸ë¦­ì´ ìˆ˜ì§‘ë˜ì§€ ì•ŠëŠ” ê²½ìš°

**ì¦ìƒ**: Prometheusì—ì„œ gpu_* ë©”íŠ¸ë¦­ì´ ë³´ì´ì§€ ì•ŠìŒ

**í•´ê²°ë°©ë²•**:
```bash
# 1. CronJob ìƒíƒœ í™•ì¸
kubectl describe cronjob gpu-session-metrics-collector

# 2. Pod ì‹¤í–‰ ë¡œê·¸ í™•ì¸  
kubectl logs -l job-name=gpu-session-metrics-collector-<latest>

# 3. Node Exporter textfile ë””ë ‰í„°ë¦¬ í™•ì¸
ls -la /var/lib/node_exporter/textfile_collector/

# 4. nvidia-smi ì ‘ê·¼ ê¶Œí•œ í™•ì¸
kubectl exec -it <gpu-pod> -- nvidia-smi
```

### 2. PIDê°€ ì¼ì¹˜í•˜ì§€ ì•ŠëŠ” ê²½ìš°

**ì¦ìƒ**: nvidia-smiì™€ Prometheusì˜ PIDê°€ ë‹¤ë¦„

**ì›ì¸**: ì´ì „ ë²„ì „ì˜ ê°€ì§œ PID ìƒì„± ë¡œì§ ì‚¬ìš©

**í•´ê²°ë°©ë²•**:
```bash
# ìµœì‹  ë²„ì „ìœ¼ë¡œ ì—…ë°ì´íŠ¸
helm upgrade gpu-monitoring . --reuse-values

# CronJob ìˆ˜ë™ ì‹¤í–‰ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
kubectl create job --from=cronjob/gpu-session-metrics-collector test-job
```

### 3. MIG í™˜ê²½ì—ì„œ GPU ì¸ì‹ ì˜¤ë¥˜

**ì¦ìƒ**: MIG ì¸ìŠ¤í„´ìŠ¤ê°€ ì˜¬ë°”ë¥´ê²Œ í‘œì‹œë˜ì§€ ì•ŠìŒ

**í™•ì¸ì‚¬í•­**:
```bash
# MIG ëª¨ë“œ ìƒíƒœ í™•ì¸
nvidia-smi --query-gpu=mig.mode.current --format=csv

# MIG ì¸ìŠ¤í„´ìŠ¤ ëª©ë¡ í™•ì¸
nvidia-smi -L | grep MIG

# ì»¨í…Œì´ë„ˆì—ì„œ MIG ì ‘ê·¼ ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
kubectl exec -it <monitoring-pod> -- nvidia-smi -L
```

## ì„±ëŠ¥ ê³ ë ¤ì‚¬í•­

### 1. ìˆ˜ì§‘ ì£¼ê¸° ì¡°ì •
```yaml
# ë†’ì€ ë¹ˆë„ (ì‹¤ì‹œê°„ì„± ì¤‘ìš”)
schedule: "*/30 * * * *"  # 30ì´ˆë§ˆë‹¤

# ì¼ë°˜ì ì¸ ì‚¬ìš©
schedule: "*/1 * * * *"   # 1ë¶„ë§ˆë‹¤ (ê¸°ë³¸ê°’)

# ë‚®ì€ ë¹ˆë„ (ë¦¬ì†ŒìŠ¤ ì ˆì•½)
schedule: "*/5 * * * *"   # 5ë¶„ë§ˆë‹¤
```

### 2. ë¦¬ì†ŒìŠ¤ í• ë‹¹
```yaml
resources:
  requests:
    memory: "64Mi"    # ìµœì†Œ ìš”êµ¬ì‚¬í•­
    cpu: "50m"
  limits:
    memory: "256Mi"   # nvidia-smi ì‹¤í–‰ì„ ìœ„í•œ ì¶©ë¶„í•œ ë©”ëª¨ë¦¬
    cpu: "200m"       # í…ìŠ¤íŠ¸ ì²˜ë¦¬ë¥¼ ìœ„í•œ CPU
```

## ë©”íŠ¸ë¦­ í™œìš© ì˜ˆì‹œ

### Grafana ëŒ€ì‹œë³´ë“œ ì¿¼ë¦¬

#### 1. GPU ì‚¬ìš©ë¥  í˜„í™©
```prometheus
# GPUë³„ í™œì„± ì„¸ì…˜ ìˆ˜
sum by (gpu) (gpu_session_count)

# ì „ì²´ GPU ì‚¬ìš©ë¥ 
(sum(gpu_session_count) / count(gpu_session_count)) * 100
```

#### 2. Podë³„ GPU ì‚¬ìš© í˜„í™©
```prometheus
# Podë³„ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
sum by (pod, namespace) (
  gpu_process_info{status="active"} * on(gpu,pid) group_left(gpu_memory) 
  gpu_process_info{status="active"}
)
```

#### 3. ìœ íœ´ GPU ê°ì§€
```prometheus
# ìœ íœ´ ìƒíƒœì¸ GPU ìˆ˜
count(gpu_session_count == 0)

# ìœ íœ´ GPU ëª©ë¡
gpu_process_info{status="idle"}
```

### ì•ŒëŒ ì„¤ì • ì˜ˆì‹œ

```yaml
# GPU ì‚¬ìš©ë¥ ì´ 90% ì´ìƒì¼ ë•Œ ì•ŒëŒ
- alert: HighGPUUtilization
  expr: (sum(gpu_session_count) / count(gpu_session_count)) > 0.9
  for: 5m
  labels:
    severity: warning
  annotations:
    summary: "GPU utilization is high ({{ $value }}%)"

# íŠ¹ì • GPUì—ì„œ ì¥ì‹œê°„ ë™ì¼ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ ì‹œ ì•ŒëŒ  
- alert: LongRunningGPUProcess
  expr: changes(gpu_process_info{status="active"}[1h]) == 0
  for: 2h
  labels:
    severity: info
  annotations:
    summary: "Long running process on GPU {{ $labels.gpu }}"
```

## ìµœì‹  ì—…ë°ì´íŠ¸ ë‚´ì—­

### v2.0 (í˜„ì¬ ë²„ì „)
- âœ… MIG(Multi-Instance GPU) ì§€ì› ì¶”ê°€
- âœ… ë™ì  GPU ê°ì§€ ê¸°ëŠ¥
- âœ… ì •í™•í•œ PID ë§¤í•‘ êµ¬í˜„
- âœ… Pod ì •ë³´ ìë™ ì—°ê²°
- âœ… UUID ê¸°ë°˜ MIG ì¸ìŠ¤í„´ìŠ¤ ì‹ë³„

### v1.0 (ì´ì „ ë²„ì „)
- âŒ ê°€ì§œ PID ìƒì„± (ìˆ˜ì •ë¨)
- âŒ í•˜ë“œì½”ë”©ëœ GPU ì¸ë±ìŠ¤ (ê°œì„ ë¨)
- âŒ MIG ë¯¸ì§€ì› (ì¶”ê°€ë¨)

---

**ë¬¸ì˜ì‚¬í•­ì´ë‚˜ ì´ìŠˆê°€ ìˆìœ¼ì‹œë©´ GitHub Issuesë‚˜ Slack #gpu-monitoring ì±„ë„ì„ í†µí•´ ì—°ë½í•´ ì£¼ì„¸ìš”.** 