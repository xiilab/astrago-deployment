# GPU ì„¸ì…˜ ëª¨ë‹ˆí„°ë§ ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ

## âš¡ 3ë¶„ë§Œì— ì‹œì‘í•˜ê¸°

### 1ë‹¨ê³„: í™œì„±í™”
```yaml
# values.yaml ë˜ëŠ” í™˜ê²½ë³„ ì„¤ì •
gpu-session-monitoring:
  enabled: true
```

### 2ë‹¨ê³„: ë°°í¬
```bash
helm upgrade --install astrago . --reuse-values
```

### 3ë‹¨ê³„: í™•ì¸
```bash
# ìˆ˜ì§‘ê¸° ë™ì‘ í™•ì¸
kubectl get cronjob gpu-session-metrics-collector

# ë©”íŠ¸ë¦­ í™•ì¸
kubectl logs -l job-name=gpu-session-metrics-collector-$(date +%s | tail -c 5)
```

## ğŸ¯ ì£¼ìš” ë©”íŠ¸ë¦­

### Prometheusì—ì„œ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì¿¼ë¦¬ë“¤

#### í˜„ì¬ GPU ì‚¬ìš© í˜„í™©
```prometheus
# GPUë³„ í™œì„± í”„ë¡œì„¸ìŠ¤ ìˆ˜
gpu_session_count

# ì „ì²´ GPU ì‚¬ìš©ë¥  (%)
(sum(gpu_session_count) / count(gpu_session_count)) * 100

# ìœ íœ´ GPU ê°œìˆ˜
count(gpu_session_count == 0)
```

#### í”„ë¡œì„¸ìŠ¤ ì •ë³´
```prometheus
# í™œì„± í”„ë¡œì„¸ìŠ¤ ëª©ë¡ (PIDì™€ ëª…ë ¹ì–´ í¬í•¨)
gpu_process_info{status="active"}

# GPUë³„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MiB)
gpu_process_info{status="active", gpu_memory!="0MiB"}
```

#### Pod ì •ë³´
```prometheus
# Podë³„ GPU ì‚¬ìš© í˜„í™©  
gpu_process_info{status="active", pod!="unknown"}

# ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë³„ GPU ì‚¬ìš©ëŸ‰
sum by (namespace) (gpu_process_info{status="active"})
```

## ğŸ” ë¬¸ì œ í•´ê²°

### ë©”íŠ¸ë¦­ì´ ì•ˆ ë³´ì¸ë‹¤ë©´?
```bash
# 1. CronJob ìƒíƒœ í™•ì¸
kubectl describe cronjob gpu-session-metrics-collector

# 2. ìµœì‹  ì‹¤í–‰ ë¡œê·¸ í™•ì¸
kubectl logs $(kubectl get pods -l job-name -o name | tail -1)

# 3. Node Exporter ì—°ê²° í™•ì¸
curl http://<node-ip>:9100/metrics | grep gpu_
```

### nvidia-smiì™€ PIDê°€ ë‹¤ë¥´ë‹¤ë©´?
```bash
# nvidia-smië¡œ ì‹¤ì œ PID í™•ì¸
nvidia-smi --query-compute-apps=pid,process_name --format=csv

# Prometheusì—ì„œ í™•ì¸
gpu_process_info{status="active"}

# ê°™ì€ PIDê°€ ë‚˜ì™€ì•¼ ì •ìƒ!
```

## ğŸ¨ Grafana ëŒ€ì‹œë³´ë“œ

### ê¸°ë³¸ íŒ¨ë„ë“¤

#### 1. GPU ì‚¬ìš©ë¥  (Stat Panel)
```prometheus
Query: (sum(gpu_session_count) / count(gpu_session_count)) * 100
Unit: Percent (0-100)
```

#### 2. GPUë³„ í™œì„± ì„¸ì…˜ (Bar Gauge)
```prometheus
Query: gpu_session_count
Legend: GPU {{gpu}}
```

#### 3. í™œì„± í”„ë¡œì„¸ìŠ¤ í…Œì´ë¸” (Table Panel)
```prometheus
Query: gpu_process_info{status="active"}
Columns: gpu, pid, command, pod, gpu_memory
```

#### 4. ì‹œê°„ë³„ ì‚¬ìš© ì¶”ì´ (Time Series)
```prometheus
Query: sum(gpu_session_count)
Title: "Total GPU Sessions Over Time"
```

## ğŸ“‹ ìì£¼ ì‚¬ìš©í•˜ëŠ” ì•ŒëŒ

### Prometheus AlertManager ê·œì¹™

```yaml
# alerts.yaml
groups:
- name: gpu-monitoring
  rules:
  # GPU ì‚¬ìš©ë¥  ë†’ìŒ
  - alert: HighGPUUsage
    expr: (sum(gpu_session_count) / count(gpu_session_count)) > 0.8
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "GPU usage is {{ $value | humanizePercentage }}"
      
  # ì¥ì‹œê°„ ì‹¤í–‰ í”„ë¡œì„¸ìŠ¤
  - alert: StuckGPUProcess  
    expr: changes(gpu_process_info{status="active"}[2h]) == 0
    for: 30m
    labels:
      severity: info
    annotations:
      summary: "Process {{ $labels.pid }} stuck on GPU {{ $labels.gpu }}"
```

## ğŸƒâ€â™‚ï¸ ê³ ê¸‰ ì‚¬ìš©ë²•

### MIG í™˜ê²½ì—ì„œ ì‚¬ìš©
```bash
# MIG í™œì„±í™” í™•ì¸
nvidia-smi --query-gpu=mig.mode.current --format=csv

# MIG ì¸ìŠ¤í„´ìŠ¤ë³„ ë©”íŠ¸ë¦­ í™•ì¸
gpu_process_info{gpu=~".*_.*_.*"}  # MIG ì¸ìŠ¤í„´ìŠ¤ (ì–¸ë”ìŠ¤ì½”ì–´ í¬í•¨)
```

### ì„±ëŠ¥ ìµœì í™”
```yaml
# ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ì´ í•„ìš”í•œ ê²½ìš°
collection:
  schedule: "*/30 * * * *"  # 30ì´ˆë§ˆë‹¤

# ë¦¬ì†ŒìŠ¤ ì ˆì•½ì´ í•„ìš”í•œ ê²½ìš°  
collection:
  schedule: "*/5 * * * *"   # 5ë¶„ë§ˆë‹¤
  resources:
    limits:
      memory: "128Mi"
      cpu: "100m"
```

## ğŸ“ ì§€ì›

- ğŸ› **ë²„ê·¸ ë¦¬í¬íŠ¸**: GitHub Issues
- ğŸ’¬ **ì§ˆë¬¸**: Slack #gpu-monitoring
- ğŸ“– **ìƒì„¸ ë¬¸ì„œ**: [GPU-Session-Monitoring-Guide.md](./GPU-Session-Monitoring-Guide.md)

---

**ğŸ‰ ì¶•í•˜í•©ë‹ˆë‹¤! ì´ì œ GPU ë¦¬ì†ŒìŠ¤ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.** 